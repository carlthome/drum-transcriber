%function [hmm,logprobs]=train(hmm,xT,lxT,nIterations,minStep);%adapts a single HMM object to an%observed sequence of (possibly vector-valued) samples,%%Input:%hmm=   single HMM object%xT=    matrix with a set of sequences of observed vectors, stored columnwise%lxT=   vector with lengths of training sub-sequences.%lxT(r)= length of r:th training sequence.%       sum(lxT) == size(xT,2)%nIterations=   (optional) min number of iterations%minStep=       (optional) min logprob improvement/training obs.vector%               = desired improvement in relative entropy/ obs.vector,%               in each iteration step.%typically, minStep=log(1.01); for 1% average relative-entropy improvement.%%Result:%hmm=      new trained HMM%logprobs= values of logprob of training obs. set from each iteration%%Method:apply methods adaptStart, adaptAccum, and adaptSet.%       the HMM must be previously initialized to the format of the%       training data.%%Arne Leijon 2004-11-23 tested%Gustav Henter 2008-08-08 (added logprobs output) tested%Arne Leijon 2009-07-24 corrected logprob bugfunction [hmm,logprobs]=train(hmm,xT,lxT,nIterations,minStep)if nargin<4    nIterations=10;%defaultend;if nargin<5    minStep=realmax;%i.e. prevent additional trainingelse    minStep=minStep*size(xT,2);%min desired increment in total logprobend;ixT=cumsum([1,lxT]);%start index for each sub-sequencelogprobs=zeros(1,nIterations);%initialize variableslogPold=-realmax;logPdelta=realmax;%logP improvement in last stepfor nTraining=1:nIterations%min number of iterations    aS=adaptStart(hmm);    for r=1:length(lxT)%each training sub-sequence separately        [aS,logP]=adaptAccum(hmm,aS,xT(:,ixT(r):(ixT(r+1)-1)));    end;    logprobs(nTraining)=logprobs(nTraining)+logP;%moved 2009-07-24    logPdelta=logprobs(nTraining)-logPold;%find latest improvement    logPold=logprobs(nTraining);    hmm=adaptSet(hmm,aS);end;if isempty(nTraining)    nTraining=0;%set nTraining to a legal value in case nIterations is 0end;while logPdelta>minStep%continue training if sufficiently good improvement    nTraining=nTraining+1;%keep track of iterations    logprobs(nTraining)=0;%initialise new logprob value    aS=adaptStart(hmm);    for r=1:length(lxT)%each training sub-sequence separately        [aS,logP]=adaptAccum(hmm,aS,xT(:,ixT(r):(ixT(r+1)-1)));    end;    logprobs(nTraining)=logprobs(nTraining)+logP;%moved 2009-07-24    logPdelta=logprobs(nTraining)-logPold;%find latest improvement    logPold=logprobs(nTraining);    hmm=adaptSet(hmm,aS);end;